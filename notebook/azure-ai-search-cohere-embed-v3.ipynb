{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "! pip install --quiet --pre azure-search-documents\n",
    "! pip install --quiet cohere python-dotenv azure-identity\n",
    "! pip install --quiet streamlit\n",
    "! pip install --quiet pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "import numpy as np\n",
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    AIStudioModelCatalogName,\n",
    "    AzureMachineLearningParameters,\n",
    "    AzureMachineLearningVectorizer,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndex,\n",
    "    SearchableField,\n",
    "    SimpleField,\n",
    "    VectorEncodingFormat,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    VectorSearchProfile\n",
    ")\n",
    "from azure.search.documents.models import (\n",
    "    VectorizableTextQuery,\n",
    "    VectorizedQuery\n",
    ")\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Cohere and Azure Credentials\n",
    "Before generating embeddings or interacting with Azure AI Search, we need to set up our credentials for both Cohere and Azure AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Azure AI Studio Cohere Configuration\n",
    "# Azure AI Studio Cohere Configuration\n",
    "AZURE_AI_STUDIO_COHERE_EMBED_KEY = os.getenv(\"AZURE_AI_STUDIO_COHERE_EMBED_KEY\")  \n",
    "AZURE_AI_STUDIO_COHERE_EMBED_ENDPOINT = os.getenv(\"AZURE_AI_STUDIO_COHERE_EMBED_ENDPOINT\")  \n",
    "AZURE_AI_STUDIO_COHERE_COMMAND_KEY = os.getenv(\"AZURE_AI_STUDIO_COHERE_COMMAND_KEY\")  \n",
    "AZURE_AI_STUDIO_COHERE_COMMAND_ENDPOINT = os.getenv(\"AZURE_AI_STUDIO_COHERE_COMMAND_ENDPOINT\")  \n",
    "SEARCH_SERVICE_API_KEY = os.getenv(\"SEARCH_SERVICE_API_KEY\")  \n",
    "SEARCH_SERVICE_ENDPOINT = os.getenv(\"SEARCH_SERVICE_ENDPOINT\")  \n",
    "BINARY_INDEX_NAME = os.getenv(\"BINARY_INDEX_NAME\")  \n",
    "INT8_INDEX_NAME = os.getenv(\"INT8_INDEX_NAME\")\n",
    "MULTIBINARY_INDEX_NAME = os.getenv(\"MULTIBINARY_INDEX_NAME\")  \n",
    "MULTIINT8_INDEX_NAME = os.getenv(\"MULTIINT8_INDEX_NAME\")  \n",
    "\n",
    "\n",
    "# Create a Cohere client using the AZURE_AI_STUDIO_COHERE_API_KEY and AZURE_AI_STUDIO_COHERE_ENDPOINT from Azure AI Studio\n",
    "cohere_azure_client = cohere.Client(\n",
    "    base_url=f\"{AZURE_AI_STUDIO_COHERE_EMBED_ENDPOINT}/v1\", \n",
    "    api_key=AZURE_AI_STUDIO_COHERE_EMBED_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-specified parameter\n",
    "USE_AAD_FOR_SEARCH = False  \n",
    "\n",
    "def authenticate_azure_search(api_key=None, use_aad_for_search=False):\n",
    "    if use_aad_for_search:\n",
    "        print(\"Using AAD for authentication.\")\n",
    "        credential = DefaultAzureCredential()\n",
    "    else:\n",
    "        print(\"Using API keys for authentication.\")\n",
    "        if api_key is None:\n",
    "            raise ValueError(\"API key must be provided if not using AAD for authentication.\")\n",
    "        credential = AzureKeyCredential(api_key)\n",
    "    return credential\n",
    "\n",
    "azure_search_credential = authenticate_azure_search(api_key=SEARCH_SERVICE_API_KEY, use_aad_for_search=USE_AAD_FOR_SEARCH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings Function\n",
    "This function `generate_embeddings` takes in a list of texts (or a single string), an input type, and an embedding type. It uses the Cohere client to generate embeddings for the provided texts using the specified model (\"embed-english-v3.0\"). The function ensures that the input texts are in a list format. If a single string is provided, it is converted into a list. The function then returns a list of embeddings.\n",
    "\n",
    "For this sample, we'll use, \"ubinary\" or \"int8\". The \"ubinary\" type is used for binary embeddings, while the \"int8\" type is used for integer embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(texts, input_type=\"search_document\", embedding_type=\"ubinary\"):\n",
    "    model = \"embed-english-v3.0\"\n",
    "\n",
    "    # Convert single string to list\n",
    "    texts = [texts] if isinstance(texts, str) else texts\n",
    "\n",
    "    response = cohere_azure_client.embed(\n",
    "        texts=texts,\n",
    "        model=model,\n",
    "        input_type=input_type,\n",
    "        embedding_types=[embedding_type],\n",
    "    )\n",
    "\n",
    "    return [embedding for embedding in getattr(response.embeddings, embedding_type)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings Function\n",
    "This function `generate_embeddings` takes in a list of texts (or a single string), an input type, and an embedding type. It uses the Cohere client to generate embeddings for the provided texts using the specified model (\"embed-multilingual-v3.0\"). The function ensures that the input texts are in a list format. If a single string is provided, it is converted into a list. The function then returns a list of embeddings.\n",
    "\n",
    "For this sample, we'll use, \"ubinary\" or \"int8\". The \"ubinary\" type is used for binary embeddings, while the \"int8\" type is used for integer embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(texts, input_type=\"search_document\", embedding_type=\"ubinary\"):\n",
    "    model = \"embed-multilingual-v3.0\"\n",
    "\n",
    "    # Convert single string to list\n",
    "    texts = [texts] if isinstance(texts, str) else texts\n",
    "\n",
    "    response = cohere_azure_client.embed(\n",
    "        texts=texts,\n",
    "        model=model,\n",
    "        input_type=input_type,\n",
    "        embedding_types=[embedding_type],\n",
    "    )\n",
    "\n",
    "    return [embedding for embedding in getattr(response.embeddings, embedding_type)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the dimension length of both the int8 and ubinary embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_embeddings(texts, input_type, embedding_type):\n",
    "    embeddings = generate_embeddings(texts, input_type=input_type, embedding_type=embedding_type)\n",
    "    print(f\"\\nLength of {input_type} embeddings ({embedding_type}):\")\n",
    "    for embedding in embeddings:\n",
    "        print(len(embedding))\n",
    "\n",
    "# Define the documents and queries\n",
    "documents = [\"Hello, world!\", \"Goodbye, world!\", \"How are you, world?\"]\n",
    "queries = [\"world\"]\n",
    "\n",
    "# Generate and print the embeddings\n",
    "generate_and_print_embeddings(documents, \"search_document\", \"ubinary\")\n",
    "generate_and_print_embeddings(documents, \"search_document\", \"int8\")\n",
    "generate_and_print_embeddings(queries, \"search_query\", \"ubinary\")\n",
    "generate_and_print_embeddings(queries, \"search_query\", \"int8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'ubinary' format uses 8-bit quantization to compress the embeddings. This means each original floating-point number in the 1024-dimensional embedding is approximated by an 8-bit unsigned integer.\n",
    "\n",
    "When these 8-bit integers are packed together, eight of them can fit in the space that was originally used by one 64-bit floating-point number (since 64 divided by 8 equals 8).\n",
    "\n",
    "This is why the 'ubinary' embeddings are 8 times smaller than the original embeddings, resulting in a 128-dimensional vector instead of a 1024-dimensional one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Update Azure AI Search Index\n",
    "This function creates or updates an Azure AI Search index to include a vector field for storing the document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_update_index(\n",
    "    client,\n",
    "    index_name,\n",
    "    vector_field_type,\n",
    "    scoring_uri,\n",
    "    authentication_key,\n",
    "    model_name,\n",
    "):\n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "        SearchField(\n",
    "            name=\"text\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            searchable=True,\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"embedding\",\n",
    "            type=vector_field_type,\n",
    "            vector_search_dimensions=1024,\n",
    "            vector_search_profile_name=\"my-vector-config\",\n",
    "            hidden=False,\n",
    "            stored=True,\n",
    "            vector_encoding_format=(\n",
    "                VectorEncodingFormat.PACKED_BIT\n",
    "                if vector_field_type == \"Collection(Edm.Byte)\"\n",
    "                else None\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    vector_search = VectorSearch(\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"my-vector-config\",\n",
    "                algorithm_configuration_name=\"my-hnsw\",\n",
    "                vectorizer=\"my-vectorizer\"\n",
    "            )\n",
    "        ],\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"my-hnsw\",\n",
    "                kind=VectorSearchAlgorithmKind.HNSW,\n",
    "                parameters=HnswParameters(\n",
    "                    metric=(\n",
    "                        VectorSearchAlgorithmMetric.HAMMING\n",
    "                        if vector_field_type == \"Collection(Edm.Byte)\"\n",
    "                        else VectorSearchAlgorithmMetric.COSINE\n",
    "                    )\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "        vectorizers=[\n",
    "            AzureMachineLearningVectorizer(\n",
    "                name=\"my-vectorizer\",\n",
    "                aml_parameters=AzureMachineLearningParameters(\n",
    "                    scoring_uri=scoring_uri,\n",
    "                    authentication_key=authentication_key,\n",
    "                    model_name=model_name,\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)\n",
    "    client.create_or_update_index(index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Documents and Their Embeddings\n",
    "Finally, this function indexes the documents along with their int8 embeddings into Azure AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_documents(search_client, documents, embeddings):\n",
    "    documents_to_index = [\n",
    "        {\"id\": str(idx), \"text\": doc, \"embedding\": emb}\n",
    "        for idx, (doc, emb) in enumerate(zip(documents, embeddings))\n",
    "    ]\n",
    "    search_client.upload_documents(documents=documents_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example documents in different languages  \n",
    "documents = [  \n",
    "    \"Alan Turing war ein englischer Mathematiker, Informatiker, Logiker, Kryptoanalytiker, Philosoph und theoretischer Biologe.\",  # German  \n",
    "    \"అలన్ ట్యూరింగ్ ఒక ఆంగ్ల గణిత శాస్త్రవేత్త, కంప్యూటర్ శాస్త్రవేత్త, తర్కవేత్త, క్రిప్టో అనలిస్ట్, తత్వవేత్త మరియు సైద్ధాంతిక జీవ శాస్త్రవేత్త.\",  # Telugu  \n",
    "    \"अलन ट्यूरिंग एक अंग्रेजी गणितज्ञ, कंप्यूटर वैज्ञानिक, तर्कशास्त्री, क्रिप्टानालिस्ट, दार्शनिक और सैद्धांतिक जीवविज्ञानी थे।\",  # Hindi  \n",
    "    \"Alan Turing fue un matemático, científico de la computación, lógico, criptanalista, filósofo y biólogo teórico inglés.\",  # Spanish  \n",
    "    \"アラン・チューリングは、イギリスの数学者、コンピュータ科学者、論理学者、暗号解読者、哲学者、理論生物学者でした。\",  # Japanese  \n",
    "    \"앨런 튜링은 영국의 수학자, 컴퓨터 과학자, 논리학자, 암호 해독자, 철학자 및 이론 생물학자였습니다.\",  # Korean  \n",
    "]  \n",
    "\n",
    "# Initialize Azure Search Index Client  \n",
    "search_index_client = SearchIndexClient(  \n",
    "    endpoint=SEARCH_SERVICE_ENDPOINT,  \n",
    "    credential=azure_search_credential  \n",
    ")  \n",
    "  \n",
    "  \n",
    "# Create the \"ubinary\" index and generate embeddings\n",
    "create_or_update_index(\n",
    "    search_index_client,\n",
    "    BINARY_INDEX_NAME,\n",
    "    \"Collection(Edm.Byte)\",\n",
    "    scoring_uri=AZURE_AI_STUDIO_COHERE_MULTIEMBED_ENDPOINT,\n",
    "    authentication_key=AZURE_AI_STUDIO_COHERE_MULTIEMBED_KEY,\n",
    "    model_name=AIStudioModelCatalogName.COHERE_EMBED_V3_ENGLISH,\n",
    ")\n",
    "\n",
    "ubinary_embeddings = generate_embeddings(\n",
    "    documents,\n",
    "    input_type=\"search_document\",\n",
    "    embedding_type=\"ubinary\"\n",
    ")\n",
    "\n",
    "ubinary_search_client = SearchClient(\n",
    "    endpoint=SEARCH_SERVICE_ENDPOINT,\n",
    "    credential=azure_search_credential,\n",
    "    index_name=BINARY_INDEX_NAME,\n",
    ")\n",
    "\n",
    "index_documents(ubinary_search_client, documents, ubinary_embeddings)\n",
    "\n",
    "# Create the \"int8\" index and generate embeddings\n",
    "create_or_update_index(\n",
    "    search_index_client,\n",
    "    INT8_INDEX_NAME,\n",
    "    \"Collection(Edm.SByte)\",\n",
    "    scoring_uri=AZURE_AI_STUDIO_COHERE_EMBED_ENDPOINT,\n",
    "    authentication_key=AZURE_AI_STUDIO_COHERE_EMBED_KEY,\n",
    "    model_name=AIStudioModelCatalogName.COHERE_EMBED_V3_ENGLISH,\n",
    ")\n",
    "\n",
    "int8_embeddings = generate_embeddings(\n",
    "    documents,\n",
    "    input_type=\"search_document\",\n",
    "    embedding_type=\"int8\"\n",
    ")\n",
    "\n",
    "int8_search_client = SearchClient(\n",
    "    endpoint=SEARCH_SERVICE_ENDPOINT,\n",
    "    credential=azure_search_credential,\n",
    "    index_name=INT8_INDEX_NAME,\n",
    ")\n",
    "\n",
    "index_documents(int8_search_client, documents, int8_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure Search Index Client\n",
    "import fitz  # PyMuPDF  \n",
    "  \n",
    "# Initialize Azure Search Index Client  \n",
    "search_index_client = SearchIndexClient(  \n",
    "    endpoint=SEARCH_SERVICE_ENDPOINT,  \n",
    "    credential=azure_search_credential  \n",
    ")  \n",
    "  \n",
    "# Function to extract text from a PDF file  \n",
    "def extract_text_from_pdf(pdf_path):  \n",
    "    doc = fitz.open(pdf_path)  \n",
    "    text = \"/Consumer Report consent.pdf\"  \n",
    "    for page in doc:  \n",
    "        text += page.get_text()  \n",
    "    return text  \n",
    "  \n",
    "# Define the paths to your PDF files  \n",
    "pdf_paths = [  \n",
    "    \"/Consumer Report consent.pdf\"\n",
    "]  \n",
    "  \n",
    "# Extract text from each PDF and store it in a list  \n",
    "documents = [extract_text_from_pdf(pdf_path) for pdf_path in pdf_paths]  \n",
    "  \n",
    "\n",
    "# Create the \"ubinary\" index and generate embeddings\n",
    "create_or_update_index(\n",
    "    search_index_client,\n",
    "    BINARY_INDEX_NAME,\n",
    "    \"Collection(Edm.Byte)\",\n",
    "    scoring_uri=AZURE_AI_STUDIO_COHERE_EMBED_ENDPOINT,\n",
    "    authentication_key=AZURE_AI_STUDIO_COHERE_EMBED_KEY,\n",
    "    model_name=AIStudioModelCatalogName.COHERE_EMBED_V3_ENGLISH,\n",
    ")\n",
    "\n",
    "ubinary_embeddings = generate_embeddings(\n",
    "    documents,\n",
    "    input_type=\"search_document\",\n",
    "    embedding_type=\"ubinary\"\n",
    ")\n",
    "\n",
    "ubinary_search_client = SearchClient(\n",
    "    endpoint=SEARCH_SERVICE_ENDPOINT,\n",
    "    credential=azure_search_credential,\n",
    "    index_name=BINARY_INDEX_NAME,\n",
    ")\n",
    "\n",
    "index_documents(ubinary_search_client, documents, ubinary_embeddings)\n",
    "\n",
    "# Create the \"int8\" index and generate embeddings\n",
    "create_or_update_index(\n",
    "    search_index_client,\n",
    "    INT8_INDEX_NAME,\n",
    "    \"Collection(Edm.SByte)\",\n",
    "    scoring_uri=AZURE_AI_STUDIO_COHERE_EMBED_ENDPOINT,\n",
    "    authentication_key=AZURE_AI_STUDIO_COHERE_EMBED_KEY,\n",
    "    model_name=AIStudioModelCatalogName.COHERE_EMBED_V3_ENGLISH,\n",
    ")\n",
    "\n",
    "int8_embeddings = generate_embeddings(\n",
    "    documents,\n",
    "    input_type=\"search_document\",\n",
    "    embedding_type=\"int8\"\n",
    ")\n",
    "\n",
    "int8_search_client = SearchClient(\n",
    "    endpoint=SEARCH_SERVICE_ENDPOINT,\n",
    "    credential=azure_search_credential,\n",
    "    index_name=INT8_INDEX_NAME,\n",
    ")\n",
    "\n",
    "index_documents(int8_search_client, documents, int8_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for vector search\n",
    "query = \"Profile Advantage Consent\"\n",
    "\n",
    "# Generate query embeddings\n",
    "# Use input_type=\"search_query\" for query embeddings if you want to leverage the function defined above for generating query embeddings\n",
    "query_embeddings = generate_embeddings(query, input_type=\"search_query\")\n",
    "\n",
    "# Note: If you do not want to use a vectorizer on your index, you can use VectorizedQuery instead of VectorizableTextQuery.\n",
    "# Here's an example:\n",
    "# vector_query_int8 = VectorizedQuery(\n",
    "#     vector=query_embeddings, k_nearest_neighbors=3, fields=\"embedding\"\n",
    "# )\n",
    "\n",
    "# Search the \"int8\" index\n",
    "vector_query_int8 = VectorizableTextQuery(\n",
    "    text=query, k_nearest_neighbors=3, fields=\"embedding\"\n",
    ")\n",
    "\n",
    "results_int8 = int8_search_client.search(\n",
    "    search_text=None,  # No search text for pure vector search\n",
    "    vector_queries=[vector_query_int8],\n",
    ")\n",
    "\n",
    "# Search the \"binary\" index\n",
    "vector_query_binary = VectorizableTextQuery(\n",
    "    text=query, k_nearest_neighbors=3, fields=\"embedding\"\n",
    ")\n",
    "\n",
    "results_binary = ubinary_search_client.search(\n",
    "    search_text=None,  # No search text for pure vector search\n",
    "    vector_queries=[vector_query_binary],\n",
    ")\n",
    "\n",
    "# Print the results side by side\n",
    "print(\"Int8 Results:\")\n",
    "for result in results_int8:\n",
    "    print(f\"Title: {result['text']}\")\n",
    "    print(f\"Score: {result['@search.score']}\\n\")\n",
    "\n",
    "print(\"Binary Results:\")\n",
    "for result in results_binary:\n",
    "    print(f\"Title: {result['text']}\")\n",
    "    print(f\"Score: {result['@search.score']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in the scores between the int8 results and the binary results is due to the use of different similarity metrics. Cosine similarity measures the similarity between numerical vectors, while Hamming distance measures the difference between binary vectors. Since cosine similarity is a measure of similarity and larger scores indicate greater similarity, the scores for the int8 results are larger than the scores for the binary results. On the other hand, since Hamming distance is a measure of dissimilarity and smaller scores indicate greater similarity, the scores for the binary results are smaller than the scores for the int8 results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Chat Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_chat = cohere.Client(\n",
    "    base_url=f\"{AZURE_AI_STUDIO_COHERE_COMMAND_ENDPOINT}/v1\", api_key=AZURE_AI_STUDIO_COHERE_COMMAND_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the documents from the search results  \n",
    "documents_binary = [{\"text\": result[\"text\"]} for result in results_binary]  \n",
    "  \n",
    "# Ground the documents from the \"binary\" index  \n",
    "chat_response_binary = co_chat.chat(  \n",
    "    message=\"Tell me about Profile Advantage Consent for Prudential\", documents=documents_binary, max_tokens=1000  \n",
    ")  \n",
    "  \n",
    "# Simulate streaming the response by breaking it into chunks  \n",
    "def stream_response_text(response_text, chunk_size=200):  \n",
    "    \"\"\"  \n",
    "    Simulates streaming by printing out the response text in chunks.  \n",
    "      \n",
    "    :param response_text: The full text response from Cohere chat.  \n",
    "    :param chunk_size: The size of each chunk to print.  \n",
    "    \"\"\"  \n",
    "    for i in range(0, len(response_text), chunk_size):  \n",
    "        print(response_text[i:i+chunk_size])  \n",
    "        # Simulate a delay to mimic streaming (optional)  \n",
    "        time.sleep(1)  # Sleep for 1 second between chunks  \n",
    "  \n",
    "# Using Binary Vectors  \n",
    "stream_response_text(chat_response_binary.text)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
